{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nAuto-tuning a convolutional network for x86 CPU\n===============================================\n**Author**: `Yao Wang <https://github.com/kevinthesun>`_, `Eddie Yan <https://github.com/eqy>`_\n\nThis is a tutorial about how to tune convolution neural network\nfor x86 cpu.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\n\nimport tvm\nfrom tvm import autotvm\nfrom tvm import relay\nfrom tvm.relay import testing\nfrom tvm.autotvm.tuner import XGBTuner, GATuner, RandomTuner, GridSearchTuner\nimport tvm.contrib.graph_runtime as runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define network\n--------------\nFirst we need to define the network in relay frontend API.\nWe can load some pre-defined network from :code:`relay.testing`.\nWe can also load models from MXNet, ONNX and TensorFlow.\n\nIn this tutorial, we choose resnet-18 as tuning example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_network(name, batch_size):\n    \"\"\"Get the symbol definition and random weight of a network\"\"\"\n    input_shape = (batch_size, 3, 224, 224)\n    output_shape = (batch_size, 1000)\n\n    if \"resnet\" in name:\n        n_layer = int(name.split('-')[1])\n        net, params = relay.testing.resnet.get_workload(num_layers=n_layer, batch_size=batch_size, dtype=dtype)\n    elif \"vgg\" in name:\n        n_layer = int(name.split('-')[1])\n        net, params = relay.testing.vgg.get_workload(num_layers=n_layer, batch_size=batch_size, dtype=dtype)\n    elif name == 'mobilenet':\n        net, params = relay.testing.mobilenet.get_workload(batch_size=batch_size, dtype=dtype)\n    elif name == 'squeezenet_v1.1':\n        net, params = relay.testing.squeezenet.get_workload(batch_size=batch_size, version='1.1', dtype=dtype)\n    elif name == 'inception_v3':\n        input_shape = (1, 3, 299, 299)\n        net, params = relay.testing.inception_v3.get_workload(batch_size=batch_size, dtype=dtype)\n    elif name == 'mxnet':\n        # an example for mxnet model\n        from mxnet.gluon.model_zoo.vision import get_model\n        block = get_model('resnet18_v1', pretrained=True)\n        net, params = relay.frontend.from_mxnet(block, shape={'data': input_shape}, dtype=dtype)\n        net = relay.Function(net.params, relay.nn.softmax(net.body), None, net.type_params, net.attrs)\n    else:\n        raise ValueError(\"Unsupported network: \" + name)\n\n    return net, params, input_shape, output_shape\n\n# Replace \"llvm\" with the correct target of your cpu.\n# For example, for AWS EC2 c5 instance with Intel Xeon\n# Platinum 8000 series, the target should be \"llvm -mcpu=skylake-avx512\".\n# For AWS EC2 c4 instance with Intel Xeon E5-2666 v3, it should be\n# \"llvm -mcpu=core-avx2\".\ntarget = \"llvm\"\n\nbatch_size = 1\ndtype = \"float32\"\nmodel_name = \"resnet-18\"\nlog_file = \"%s.log\" % model_name\n\n# Set number of threads used for tuning based on the number of\n# physical cpu cores on your machine.\nnum_threads = 1\nos.environ[\"TVM_NUM_THREADS\"] = str(num_threads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configure tensor tuning settings and create tasks\n-------------------------------------------------\nTo get better kernel execution performance on x86 cpu,\nwe need to change data layout of convolution kernel from\n\"NCHW\" to \"NCHWc\". To deal with this situation, we define\nconv2d_NCHWc operator in topi. We will tune this operator\ninstead of plain conv2d.\n\nWe will use local mode for tuning configuration. RPC tracker\nmode can be setup similarly to the approach in\n`tune_relay_arm` tutorial.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tuning_option = {\n    'log_filename': log_file,\n    'tuner': 'random',\n    'early_stopping': None,\n\n    'measure_option': autotvm.measure_option(\n        builder=autotvm.LocalBuilder(),\n        runner=autotvm.LocalRunner(number=10, repeat=1,\n                                   min_repeat_ms=1000),\n    ),\n}\n\n# You can skip the implementation of this function for this tutorial.\ndef tune_kernels(tasks,\n                 measure_option,\n                 tuner='gridsearch',\n                 early_stopping=None,\n                 log_filename='tuning.log'):\n\n    for i, tsk in enumerate(tasks):\n        prefix = \"[Task %2d/%2d] \" % (i+1, len(tasks))\n\n        # converting conv2d tasks to conv2d_NCHWc tasks\n        op_name = tsk.workload[0]\n        if op_name == 'conv2d':\n            func_create = 'topi_x86_conv2d_NCHWc'\n        elif op_name == 'depthwise_conv2d_nchw':\n            func_create = 'topi_x86_depthwise_conv2d_NCHWc_from_nchw'\n        else:\n            raise ValueError(\"Tuning {} is not supported on x86\".format(op_name))\n\n        task = autotvm.task.create(func_create, args=tsk.args,\n                                   target=target, template_key='direct')\n        task.workload = tsk.workload\n\n        # create tuner\n        if tuner == 'xgb' or tuner == 'xgb-rank':\n            tuner_obj = XGBTuner(task, loss_type='rank')\n        elif tuner == 'ga':\n            tuner_obj = GATuner(task, pop_size=50)\n        elif tuner == 'random':\n            tuner_obj = RandomTuner(task)\n        elif tuner == 'gridsearch':\n            tuner_obj = GridSearchTuner(task)\n        else:\n            raise ValueError(\"Invalid tuner: \" + tuner)\n\n        # do tuning\n        n_trial=len(task.config_space)\n        tuner_obj.tune(n_trial=n_trial,\n                       early_stopping=early_stopping,\n                       measure_option=measure_option,\n                       callbacks=[\n                           autotvm.callback.progress_bar(n_trial, prefix=prefix),\n                           autotvm.callback.log_to_file(log_filename)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we launch tuning jobs and evaluate the end-to-end performance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def tune_and_evaluate(tuning_opt):\n    # extract workloads from relay program\n    print(\"Extract tasks...\")\n    net, params, data_shape, out_shape = get_network(model_name, batch_size)\n    tasks = autotvm.task.extract_from_program(net, target=target,\n                                              params=params, ops=(relay.op.nn.conv2d,))\n\n    # run tuning tasks\n    print(\"Tuning...\")\n    tune_kernels(tasks, **tuning_opt)\n\n    # compile kernels with history best records\n    with autotvm.apply_history_best(log_file):\n        print(\"Compile...\")\n        with relay.build_config(opt_level=3):\n            graph, lib, params = relay.build_module.build(\n                net, target=target,  params=params)\n\n        # upload parameters to device\n        ctx = tvm.cpu()\n        data_tvm = tvm.nd.array((np.random.uniform(size=data_shape)).astype(dtype))\n        module = runtime.create(graph, lib, ctx)\n        module.set_input('data', data_tvm)\n        module.set_input(**params)\n\n        # evaluate\n        print(\"Evaluate inference time cost...\")\n        ftimer = module.module.time_evaluator(\"run\", ctx, number=100, repeat=3)\n        prof_res = np.array(ftimer().results) * 1000  # convert to millisecond\n        print(\"Mean inference time (std dev): %.2f ms (%.2f ms)\" %\n              (np.mean(prof_res), np.std(prof_res)))\n\n# We do not run the tuning in our webpage server since it takes too long.\n# Uncomment the following line to run it by yourself.\n\n# tune_and_evaluate(tuning_option)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sample Output\n-------------\nThe tuning needs to compile many programs and extract feature from them.\nSo a high performance CPU is recommended.\nOne sample output is listed below.\n\n.. code-block:: bash\n\n   Extract tasks...\n   Tuning...\n   [Task  1/12]  Current/Best:  598.05/2497.63 GFLOPS | Progress: (252/252) | 1357.95 s Done.\n   [Task  2/12]  Current/Best:  522.63/2279.24 GFLOPS | Progress: (784/784) | 3989.60 s Done.\n   [Task  3/12]  Current/Best:  447.33/1927.69 GFLOPS | Progress: (784/784) | 3869.14 s Done.\n   [Task  4/12]  Current/Best:  481.11/1912.34 GFLOPS | Progress: (672/672) | 3274.25 s Done.\n   [Task  5/12]  Current/Best:  414.09/1598.45 GFLOPS | Progress: (672/672) | 2720.78 s Done.\n   [Task  6/12]  Current/Best:  508.96/2273.20 GFLOPS | Progress: (768/768) | 3718.75 s Done.\n   [Task  7/12]  Current/Best:  469.14/1955.79 GFLOPS | Progress: (576/576) | 2665.67 s Done.\n   [Task  8/12]  Current/Best:  230.91/1658.97 GFLOPS | Progress: (576/576) | 2435.01 s Done.\n   [Task  9/12]  Current/Best:  487.75/2295.19 GFLOPS | Progress: (648/648) | 3009.95 s Done.\n   [Task 10/12]  Current/Best:  182.33/1734.45 GFLOPS | Progress: (360/360) | 1755.06 s Done.\n   [Task 11/12]  Current/Best:  372.18/1745.15 GFLOPS | Progress: (360/360) | 1684.50 s Done.\n   [Task 12/12]  Current/Best:  215.34/2271.11 GFLOPS | Progress: (400/400) | 2128.74 s Done.\n   Compile...\n   Evaluate inference time cost...\n   Mean inference time (std dev): 3.16 ms (0.03 ms)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}