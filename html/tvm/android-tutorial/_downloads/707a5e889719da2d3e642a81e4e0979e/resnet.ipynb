{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nResNet Inference Example\n========================\n**Author**: `Thierry Moreau <https://homes.cs.washington.edu/~moreau/>`_\n\nThis tutorial provides an end-to-end demo, on how to run ResNet-18 inference\nonto the VTA accelerator design to perform ImageNet classification tasks.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import Libraries\n----------------\nWe start by importing the tvm, vta, nnvm libraries to run this example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, print_function\n\nimport os\nimport time\nfrom io import BytesIO\n\nimport numpy as np\nimport requests\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\n\nimport tvm\nfrom tvm import rpc, autotvm\nfrom tvm.contrib import graph_runtime, util\nfrom tvm.contrib.download import download\nimport nnvm.compiler\nimport vta\nimport vta.testing\n\n# Load VTA parameters from the vta/config/vta_config.json file\nenv = vta.get_env()\n\n# Helper to crop an image to a square (224, 224)\n# Takes in an Image object, returns an Image object\ndef thumbnailify(image, pad=15):\n    w, h = image.size\n    crop = ((w-h)//2+pad, pad, h+(w-h)//2-pad, h-pad)\n    image = image.crop(crop)\n    image = image.resize((224, 224))\n    return image\n\n# Helper function to read in image\n# Takes in Image object, returns an ND array\ndef process_image(image):\n    # Convert to neural network input format\n    image = np.array(image) - np.array([123., 117., 104.])\n    image /= np.array([58.395, 57.12, 57.375])\n    image = image.transpose((2, 0, 1))\n    image = image[np.newaxis, :]\n\n    return tvm.nd.array(image.astype(\"float32\"))\n\n# Classification helper function\n# Takes in the graph runtime, and an image, and returns top result and time\ndef classify(m, image):\n    m.set_input('data', image)\n    timer = m.module.time_evaluator(\"run\", ctx, number=1)\n    tcost = timer()\n    tvm_output = m.get_output(0)\n    top = np.argmax(tvm_output.asnumpy()[0])\n    tcost = \"t={0:.2f}s\".format(tcost.mean)\n    return tcost + \" {}\".format(synset[top])\n\n# Helper function to compile the NNVM graph\n# Takes in a path to a graph file, params file, and device target\n# Returns the NNVM graph object, a compiled library object, and the params dict\ndef generate_graph(graph_fn, params_fn, device=\"vta\"):\n    # Measure build start time\n    build_start = time.time()\n\n    # Derive the TVM target\n    target = tvm.target.create(\"llvm -device={}\".format(device))\n\n    # Derive the LLVM compiler flags\n    # When targetting the Pynq, cross-compile to ARMv7 ISA\n    if env.TARGET == \"sim\":\n        target_host = \"llvm\"\n    elif env.TARGET == \"pynq\":\n        target_host = \"llvm -mtriple=armv7-none-linux-gnueabihf -mcpu=cortex-a9 -mattr=+neon\"\n\n    # Load the ResNet-18 graph and parameters\n    sym = nnvm.graph.load_json(open(graph_fn).read())\n    params = nnvm.compiler.load_param_dict(open(params_fn, 'rb').read())\n\n    # Populate the shape and data type dictionary\n    shape_dict = {\"data\": (1, 3, 224, 224)}\n    dtype_dict = {\"data\": 'float32'}\n    shape_dict.update({k: v.shape for k, v in params.items()})\n    dtype_dict.update({k: str(v.dtype) for k, v in params.items()})\n\n    # Apply NNVM graph optimization passes\n    sym = vta.graph.clean_cast(sym)\n    sym = vta.graph.clean_conv_fuse(sym)\n    if target.device_name == \"vta\":\n        assert env.BLOCK_IN == env.BLOCK_OUT\n        sym = vta.graph.pack(sym, shape_dict, env.BATCH, env.BLOCK_OUT)\n\n    # Compile NNVM graph\n    with nnvm.compiler.build_config(opt_level=3):\n        if target.device_name != \"vta\":\n            graph, lib, params = nnvm.compiler.build(\n                sym, target, shape_dict, dtype_dict,\n                params=params, target_host=target_host)\n        else:\n            with vta.build_config():\n                graph, lib, params = nnvm.compiler.build(\n                    sym, target, shape_dict, dtype_dict,\n                    params=params, target_host=target_host)\n\n    # Save the compiled inference graph library\n    assert tvm.module.enabled(\"rpc\")\n    temp = util.tempdir()\n    lib.save(temp.relpath(\"graphlib.o\"))\n\n    # Send the inference library over to the remote RPC server\n    remote.upload(temp.relpath(\"graphlib.o\"))\n    lib = remote.load_module(\"graphlib.o\")\n\n    # Measure build time\n    build_time = time.time() - build_start\n    print(\"ResNet-18 inference graph built in {0:.2f}s!\".format(build_time))\n\n    return graph, lib, params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download ResNet Model\n--------------------------------------------\nDownload the necessary files to run ResNet-18.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Obtain ResNet model and download them into _data dir\nurl = \"https://github.com/uwsaml/web-data/raw/master/vta/models/\"\ncateg_fn = 'synset.txt'\ngraph_fn = 'resnet18_qt8.json'\nparams_fn = 'resnet18_qt8.params'\n\n# Create data dir\ndata_dir = \"_data/\"\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\n# Download files\nfor file in [categ_fn, graph_fn, params_fn]:\n    download(os.path.join(url, file), os.path.join(data_dir, file))\n\n# Read in ImageNet Categories\nsynset = eval(open(os.path.join(data_dir, categ_fn)).read())\n\n# Download pre-tuned op parameters of conv2d for ARM CPU used in VTA\nautotvm.tophub.check_backend('vta')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup the Pynq Board's RPC Server\n---------------------------------\nBuild the RPC server's VTA runtime and program the Pynq FPGA.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Measure build start time\nreconfig_start = time.time()\n\n# We read the Pynq RPC host IP address and port number from the OS environment\nhost = os.environ.get(\"VTA_PYNQ_RPC_HOST\", \"192.168.2.99\")\nport = int(os.environ.get(\"VTA_PYNQ_RPC_PORT\", \"9091\"))\n\n# We configure both the bitstream and the runtime system on the Pynq\n# to match the VTA configuration specified by the vta_config.json file.\nif env.TARGET == \"pynq\":\n    # Make sure that TVM was compiled with RPC=1\n    assert tvm.module.enabled(\"rpc\")\n    remote = rpc.connect(host, port)\n\n    # Reconfigure the JIT runtime\n    vta.reconfig_runtime(remote)\n\n    # Program the FPGA with a pre-compiled VTA bitstream.\n    # You can program the FPGA with your own custom bitstream\n    # by passing the path to the bitstream file instead of None.\n    vta.program_fpga(remote, bitstream=None)\n\n    # Report on reconfiguration time\n    reconfig_time = time.time() - reconfig_start\n    print(\"Reconfigured FPGA and RPC runtime in {0:.2f}s!\".format(reconfig_time))\n\n# In simulation mode, host the RPC server locally.\nelif env.TARGET == \"sim\":\n    remote = rpc.LocalSession()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the ResNet Runtime\n------------------------\nBuild the ResNet graph runtime, and configure the parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set ``device=vtacpu`` to run inference on the CPU\n# or ``device=vta`` to run inference on the FPGA.\ndevice = \"vta\"\n\n# Device context\nctx = remote.ext_dev(0) if device == \"vta\" else remote.cpu(0)\n\n# Build the graph runtime\ngraph, lib, params = generate_graph(os.path.join(data_dir, graph_fn),\n                                    os.path.join(data_dir, params_fn),\n                                    device)\nm = graph_runtime.create(graph, lib, ctx)\n\n# Set the parameters\nm.set_input(**params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run ResNet-18 inference on a sample image\n-----------------------------------------\nPerform image classification on test image.\nYou can change the test image URL to any image of your choosing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Read in test image\nimage_url = 'https://homes.cs.washington.edu/~moreau/media/vta/cat.jpg'\n# Read in test image\nresponse = requests.get(image_url)\nimage = Image.open(BytesIO(response.content)).resize((224, 224))\n# Show Image\nplt.imshow(image)\nplt.show()\n# Set the input\nimage = process_image(image)\nm.set_input('data', image)\n\n# Perform inference\ntimer = m.module.time_evaluator(\"run\", ctx, number=1)\ntcost = timer()\n\n# Get classification results\ntvm_output = m.get_output(0)\ntop_categories = np.argsort(tvm_output.asnumpy()[0])\n\n# Report top-5 classification results\nprint(\"ResNet-18 Prediction #1:\", synset[top_categories[-1]])\nprint(\"                     #2:\", synset[top_categories[-2]])\nprint(\"                     #3:\", synset[top_categories[-3]])\nprint(\"                     #4:\", synset[top_categories[-4]])\nprint(\"                     #5:\", synset[top_categories[-5]])\nprint(\"Performed inference in {0:.2f}s\".format(tcost.mean))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run a Youtube Video Image Classifier\n------------------------------------\nPerform image classification on test stream on 1 frame every 48 frames.\nComment the `if False:` out to run the demo\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Early exit - remove for Demo\nif False:\n\n    import cv2\n    import pafy\n    from IPython.display import clear_output\n\n    # Helper to crop an image to a square (224, 224)\n    # Takes in an Image object, returns an Image object\n    def thumbnailify(image, pad=15):\n        w, h = image.size\n        crop = ((w-h)//2+pad, pad, h+(w-h)//2-pad, h-pad)\n        image = image.crop(crop)\n        image = image.resize((224, 224))\n        return image\n\n    # 16:16 inches\n    plt.rcParams['figure.figsize'] = [16, 16]\n\n    # Stream the video in\n    url = \"https://www.youtube.com/watch?v=PJlmYh27MHg&t=2s\"\n    video = pafy.new(url)\n    best = video.getbest(preftype=\"mp4\")\n    cap = cv2.VideoCapture(best.url)\n\n    # Process one frame out of every 48 for variety\n    count = 0\n    guess = \"\"\n    while(count<2400):\n\n        # Capture frame-by-frame\n        ret, frame = cap.read()\n\n        # Process one every 48 frames\n        if count % 48 == 1:\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame = Image.fromarray(frame)\n            # Crop and resize\n            thumb = np.array(thumbnailify(frame))\n            image = process_image(thumb)\n            guess = classify(m, image)\n\n            # Insert guess in frame\n            frame = cv2.rectangle(thumb,(0,0),(200,0),(0,0,0),50)\n            cv2.putText(frame, guess, (5,15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (256,256,256), 1, cv2.LINE_AA)\n\n            plt.imshow(thumb)\n            plt.axis('off')\n            plt.show()\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n            clear_output(wait=True)\n\n        count += 1\n\n    # When everything done, release the capture\n    cap.release()\n    cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}