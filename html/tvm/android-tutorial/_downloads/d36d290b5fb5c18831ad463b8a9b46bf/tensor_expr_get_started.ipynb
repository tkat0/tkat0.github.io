{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nGet Started with Tensor Expression\n==================================\n**Author**: `Tianqi Chen <https://tqchen.github.io>`_\n\nThis is an introduction tutorial to Tensor expression language in TVM.\nTVM uses a domain specific tensor expression for efficient kernel construction.\n\nIn this tutorial, we will demonstrate the basic workflow to use\nthe tensor expression language.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, print_function\n\nimport tvm\nimport numpy as np\n\n# Global declarations of environment.\n\ntgt_host=\"llvm\"\n# Change it to respective GPU if gpu is enabled Ex: cuda, opencl\ntgt=\"cuda\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vector Add Example\n------------------\nIn this tutorial, we will use a vector addition example to demonstrate\nthe workflow.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Describe the Computation\n------------------------\nAs a first step, we need to describe our computation.\nTVM adopts tensor semantics, with each intermediate result\nrepresented as multi-dimensional array. The user need to describe\nthe computation rule that generate the tensors.\n\nWe first define a symbolic variable n to represent the shape.\nWe then define two placeholder Tensors, A and B, with given shape (n,)\n\nWe then describe the result tensor C, with a compute operation.\nThe compute function takes the shape of the tensor, as well as a lambda function\nthat describes the computation rule for each position of the tensor.\n\nNo computation happens during this phase, as we are only declaring how\nthe computation should be done.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = tvm.var(\"n\")\nA = tvm.placeholder((n,), name='A')\nB = tvm.placeholder((n,), name='B')\nC = tvm.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\nprint(type(C))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Schedule the Computation\n------------------------\nWhile the above lines describes the computation rule, we can compute\nC in many ways since the axis of C can be computed in data parallel manner.\nTVM asks user to provide a description of computation called schedule.\n\nA schedule is a set of transformation of computation that transforms\nthe loop of computations in the program.\n\nAfter we construct the schedule, by default the schedule computes\nC in a serial manner in a row-major order.\n\n.. code-block:: c\n\n  for (int i = 0; i < n; ++i) {\n    C[i] = A[i] + B[i];\n  }\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s = tvm.create_schedule(C.op)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We used the split construct to split the first axis of C,\nthis will split the original iteration axis into product of\ntwo iterations. This is equivalent to the following code.\n\n.. code-block:: c\n\n  for (int bx = 0; bx < ceil(n / 64); ++bx) {\n    for (int tx = 0; tx < 64; ++tx) {\n      int i = bx * 64 + tx;\n      if (i < n) {\n        C[i] = A[i] + B[i];\n      }\n    }\n  }\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bx, tx = s[C].split(C.op.axis[0], factor=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we bind the iteration axis bx and tx to threads in the GPU\ncompute grid. These are GPU specific constructs that allows us\nto generate code that runs on GPU.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if tgt == \"cuda\" or tgt.startswith('opencl'):\n  s[C].bind(bx, tvm.thread_axis(\"blockIdx.x\"))\n  s[C].bind(tx, tvm.thread_axis(\"threadIdx.x\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compilation\n-----------\nAfter we have finished specifying the schedule, we can compile it\ninto a TVM function. By default TVM compiles into a type-erased\nfunction that can be directly called from python side.\n\nIn the following line, we use tvm.build to create a function.\nThe build function takes the schedule, the desired signature of the\nfunction(including the inputs and outputs) as well as target language\nwe want to compile to.\n\nThe result of compilation fadd is a GPU device function(if GPU is involved)\nthat can as well as a host wrapper that calls into the GPU function.\nfadd is the generated host wrapper function, it contains reference\nto the generated device function internally.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fadd = tvm.build(s, [A, B, C], tgt, target_host=tgt_host, name=\"myadd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the Function\n----------------\nThe compiled function TVM function is designed to be a concise C API\nthat can be invoked from any languages.\n\nWe provide an minimum array API in python to aid quick testing and prototyping.\nThe array API is based on `DLPack <https://github.com/dmlc/dlpack>`_ standard.\n\n- We first create a gpu context.\n- Then tvm.nd.array copies the data to gpu.\n- fadd runs the actual computation.\n- asnumpy() copies the gpu array back to cpu and we can use this to verify correctness\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ctx = tvm.context(tgt, 0)\n\nn = 1024\na = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), ctx)\nb = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), ctx)\nc = tvm.nd.array(np.zeros(n, dtype=C.dtype), ctx)\nfadd(a, b, c)\ntvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect the Generated Code\n--------------------------\nYou can inspect the generated code in TVM. The result of tvm.build\nis a tvm Module. fadd is the host module that contains the host wrapper,\nit also contains a device module for the CUDA (GPU) function.\n\nThe following code fetches the device module and prints the content code.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if tgt == \"cuda\" or tgt.startswith('opencl'):\n    dev_module = fadd.imported_modules[0]\n    print(\"-----GPU code-----\")\n    print(dev_module.get_source())\nelse:\n    print(fadd.get_source())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Code Specialization\n\n  As you may noticed, during the declaration, A, B and C both\n  takes the same shape argument n. TVM will take advantage of this\n  to pass only single shape argument to the kernel, as you will find in\n  the printed device code. This is one form of specialization.\n\n  On the host side, TVM will automatically generate check code\n  that checks the constraints in the parameters. So if you pass\n  arrays with different shapes into the fadd, an error will be raised.\n\n  We can do more specializations. For example, we can write\n  :code:`n = tvm.convert(1024)` instead of :code:`n = tvm.var(\"n\")`,\n  in the computation declaration. The generated function will\n  only take vectors with length 1024.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save Compiled Module\n--------------------\nBesides runtime compilation, we can save the compiled modules into\nfile and load them back later. This is called ahead of time compilation.\n\nThe following code first does the following step:\n\n- It saves the compiled host module into an object file.\n- Then it saves the device module into a ptx file.\n- cc.create_shared calls a env compiler(gcc) to create a shared library\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.contrib import cc\nfrom tvm.contrib import util\n\ntemp = util.tempdir()\nfadd.save(temp.relpath(\"myadd.o\"))\nif tgt == \"cuda\":\n    fadd.imported_modules[0].save(temp.relpath(\"myadd.ptx\"))\nif tgt.startswith('opencl'):\n    fadd.imported_modules[0].save(temp.relpath(\"myadd.cl\"))\ncc.create_shared(temp.relpath(\"myadd.so\"), [temp.relpath(\"myadd.o\")])\nprint(temp.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Module Storage Format\n\n  The CPU(host) module is directly saved as a shared library(so).\n  There can be multiple customized format on the device code.\n  In our example, device code is stored in ptx, as well as a meta\n  data json file. They can be loaded and linked separately via import.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Compiled Module\n--------------------\nWe can load the compiled module from the file system and run the code.\nThe following code load the host and device module separately and\nre-link them together. We can verify that the newly loaded function works.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fadd1 = tvm.module.load(temp.relpath(\"myadd.so\"))\nif tgt == \"cuda\":\n    fadd1_dev = tvm.module.load(temp.relpath(\"myadd.ptx\"))\n    fadd1.import_module(fadd1_dev)\n\nif tgt.startswith('opencl'):\n    fadd1_dev = tvm.module.load(temp.relpath(\"myadd.cl\"))\n    fadd1.import_module(fadd1_dev)\n\nfadd1(a, b, c)\ntvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pack Everything into One Library\n--------------------------------\nIn the above example, we store the device and host code separately.\nTVM also supports export everything as one shared library.\nUnder the hood, we pack the device modules into binary blobs and link\nthem together with the host code.\nCurrently we support packing of Metal, OpenCL and CUDA modules.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fadd.export_library(temp.relpath(\"myadd_pack.so\"))\nfadd2 = tvm.module.load(temp.relpath(\"myadd_pack.so\"))\nfadd2(a, b, c)\ntvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Runtime API and Thread-Safety\n\n  The compiled modules of TVM do not depend on the TVM compiler.\n  Instead, it only depends on a minimum runtime library.\n  TVM runtime library wraps the device drivers and provides\n  thread-safe and device agnostic call into the compiled functions.\n\n  This means you can call the compiled TVM function from any thread,\n  on any GPUs.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate OpenCL Code\n--------------------\nTVM provides code generation features into multiple backends,\nwe can also generate OpenCL code or LLVM code that runs on CPU backends.\n\nThe following codeblocks generate opencl code, creates array on opencl\ndevice, and verifies the correctness of the code.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if tgt.startswith('opencl'):\n    fadd_cl = tvm.build(s, [A, B, C], tgt, name=\"myadd\")\n    print(\"------opencl code------\")\n    print(fadd_cl.imported_modules[0].get_source())\n    ctx = tvm.cl(0)\n    n = 1024\n    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), ctx)\n    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), ctx)\n    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), ctx)\n    fadd_cl(a, b, c)\n    tvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary\n-------\nThis tutorial provides a walk through of TVM workflow using\na vector add example. The general workflow is\n\n- Describe your computation via series of operations.\n- Describe how we want to compute use schedule primitives.\n- Compile to the target function we want.\n- Optionally, save the function to be loaded later.\n\nYou are more than welcomed to checkout other examples and\ntutorials to learn more about the supported operations, schedule primitives\nand other features in TVM.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}