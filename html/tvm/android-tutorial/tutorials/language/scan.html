

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Scan and Recurrent Kernel &mdash; tvm 0.6.dev documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tvm_theme.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Intrinsics and Math Functions" href="intrin_math.html" />
    <link rel="prev" title="Reduction" href="reduction.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/tvm-logo-small.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.6.dev
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../relay_quick_start.html">Quick Start Tutorial for Compiling Deep Learning Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cross_compilation_and_rpc.html">Cross Compilation and RPC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tensor_expr_get_started.html">Get Started with Tensor Expression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#compile-deep-learning-models">Compile Deep Learning Models</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#tensor-expression-and-schedules">Tensor Expression and Schedules</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="extern_op.html">External Tensor Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="tuple_inputs.html">Compute and Reduce with Tuple Inputs</a></li>
<li class="toctree-l3"><a class="reference internal" href="reduction.html">Reduction</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Scan and Recurrent Kernel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#schedule-the-scan-cell">Schedule the Scan Cell</a></li>
<li class="toctree-l4"><a class="reference internal" href="#build-and-verify">Build and Verify</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-stage-scan-cell">Multi-Stage Scan Cell</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multiple-states">Multiple States</a></li>
<li class="toctree-l4"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="intrin_math.html">Intrinsics and Math Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="schedule_primitives.html">Schedule Primitives in TVM</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensorize.html">Use Tensorize to Leverage Hardware Intrinsics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#optimize-tensor-operators">Optimize Tensor Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#auto-tuning">Auto tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#developer-tutorials">Developer Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#topi-tvm-operator-inventory">TOPI: TVM Operator Inventory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../vta/index.html">VTA: Deep Learning Accelerator Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy/index.html">Deploy and Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/index.html">Contribute to TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">Frequently Asked Questions</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../langref/index.html">Language Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_links.html">Links to API References</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dev/index.html">Design and Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nnvm_top.html">NNVM Core Tensor Operators</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">tvm</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Tutorials</a> &raquo;</li>
        
      <li>Scan and Recurrent Kernel</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/tutorials/language/scan.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Click <a class="reference internal" href="#sphx-glr-download-tutorials-language-scan-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="scan-and-recurrent-kernel">
<span id="sphx-glr-tutorials-language-scan-py"></span><h1>Scan and Recurrent Kernel<a class="headerlink" href="#scan-and-recurrent-kernel" title="Permalink to this headline">¶</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://tqchen.github.io">Tianqi Chen</a></p>
<p>This is an introduction material on how to do recurrent computing in TVM.
Recurrent computing is a typical pattern in neural networks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</pre></div>
</div>
<p>TVM supports a scan operator to describe symbolic loop.
The following scan op computes cumsum over columns of X.</p>
<p>The scan is carried over the highest dimension of the tensor.
<code class="code docutils literal notranslate"><span class="pre">s_state</span></code> is a placeholder that describes the transition state of the scan.
<code class="code docutils literal notranslate"><span class="pre">s_init</span></code> describes how we can initialize the first k timesteps.
Here since s_init’s first dimension is 1, it describes how we initialize
The state at first timestep.</p>
<p><code class="code docutils literal notranslate"><span class="pre">s_update</span></code> describes how to update the value at timestep t. The update
value can refer back to the values of previous timestep via state placeholder.
Note that while it is invalid to refer to <code class="code docutils literal notranslate"><span class="pre">s_state</span></code> at current or later timestep.</p>
<p>The scan takes in state placeholder, initial value and update description.
It is also recommended(although not necessary) to list the inputs to the scan cell.
The result of the scan is a tensor, giving the result of <code class="code docutils literal notranslate"><span class="pre">s_state</span></code> after the
update over the time domain.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;n&quot;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">s_state</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">s_init</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
<span class="n">s_update</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="n">s_state</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
<span class="n">s_scan</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">s_init</span><span class="p">,</span> <span class="n">s_update</span><span class="p">,</span> <span class="n">s_state</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">X</span><span class="p">])</span>
</pre></div>
</div>
<div class="section" id="schedule-the-scan-cell">
<h2>Schedule the Scan Cell<a class="headerlink" href="#schedule-the-scan-cell" title="Permalink to this headline">¶</a></h2>
<p>We can schedule the body of the scan by scheduling the update and
init part seperately. Note that it is invalid to schedule the
first iteration dimension of the update part.
To split on the time iteration, user can schedule on scan_op.scan_axis instead.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">create_schedule</span><span class="p">(</span><span class="n">s_scan</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
<span class="n">num_thread</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">block_x</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">thread_axis</span><span class="p">(</span><span class="s2">&quot;blockIdx.x&quot;</span><span class="p">)</span>
<span class="n">thread_x</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">thread_axis</span><span class="p">(</span><span class="s2">&quot;threadIdx.x&quot;</span><span class="p">)</span>
<span class="n">xo</span><span class="p">,</span> <span class="n">xi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">s_init</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">s_init</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">factor</span><span class="o">=</span><span class="n">num_thread</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">s_init</span><span class="p">]</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">xo</span><span class="p">,</span> <span class="n">block_x</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">s_init</span><span class="p">]</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">thread_x</span><span class="p">)</span>
<span class="n">xo</span><span class="p">,</span> <span class="n">xi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">s_update</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">s_update</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">factor</span><span class="o">=</span><span class="n">num_thread</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">s_update</span><span class="p">]</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">xo</span><span class="p">,</span> <span class="n">block_x</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">s_update</span><span class="p">]</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">thread_x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">tvm</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">s_scan</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>produce scan {
  // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = ((n + 255)/256)
  // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 256
  if (likely(((blockIdx.x*256) &lt; (n - threadIdx.x)))) {
    scan[((blockIdx.x*256) + threadIdx.x)] = X[((blockIdx.x*256) + threadIdx.x)]
  }
  for (scan.idx, 0, (m + -1)) {
    // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = ((n + 255)/256)
    // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 256
    if (likely(((blockIdx.x*256) &lt; (n - threadIdx.x)))) {
      scan[(((blockIdx.x*256) + ((scan.idx + 1)*n)) + threadIdx.x)] = (scan[(((blockIdx.x*256) + (scan.idx*n)) + threadIdx.x)] + X[(((blockIdx.x*256) + ((scan.idx + 1)*n)) + threadIdx.x)])
    }
  }
}
</pre></div>
</div>
</div>
<div class="section" id="build-and-verify">
<h2>Build and Verify<a class="headerlink" href="#build-and-verify" title="Permalink to this headline">¶</a></h2>
<p>We can build the scan kernel like other tvm kernels, here we use
numpy to verify the correctness of the result.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fscan</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">s_scan</span><span class="p">],</span> <span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;myscan&quot;</span><span class="p">)</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">gpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">a_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">s_scan</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">a_np</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">s_scan</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">ctx</span><span class="p">)</span>
<span class="n">fscan</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">tvm</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">a_np</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="multi-stage-scan-cell">
<h2>Multi-Stage Scan Cell<a class="headerlink" href="#multi-stage-scan-cell" title="Permalink to this headline">¶</a></h2>
<p>In the above example we described the scan cell using one Tensor
computation stage in s_update. It is possible to use multiple
Tensor stages in the scan cell.</p>
<p>The following lines demonstrate a scan with two stage operations
in the scan cell.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;n&quot;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">s_state</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">s_init</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
<span class="n">s_update_s1</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="n">s_state</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;s1&quot;</span><span class="p">)</span>
<span class="n">s_update_s2</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="n">s_update_s1</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;s2&quot;</span><span class="p">)</span>
<span class="n">s_scan</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">s_init</span><span class="p">,</span> <span class="n">s_update_s2</span><span class="p">,</span> <span class="n">s_state</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">X</span><span class="p">])</span>
</pre></div>
</div>
<p>These intermediate tensors can also be scheduled normally.
To ensure correctness, TVM creates a group constraint to forbid
the body of scan to be compute_at locations outside the scan loop.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">create_schedule</span><span class="p">(</span><span class="n">s_scan</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
<span class="n">xo</span><span class="p">,</span> <span class="n">xi</span> <span class="o">=</span> <span class="n">s</span><span class="p">[</span><span class="n">s_update_s2</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">s_update_s2</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">factor</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">s</span><span class="p">[</span><span class="n">s_update_s1</span><span class="p">]</span><span class="o">.</span><span class="n">compute_at</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">s_update_s2</span><span class="p">],</span> <span class="n">xo</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">tvm</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">s_scan</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>// attr [s1] storage_scope = &quot;global&quot;
allocate s1[float32 * 32]
produce scan {
  for (i, 0, n) {
    scan[i] = X[i]
  }
  for (scan.idx, 0, (m + -1)) {
    for (i.outer, 0, ((n + 31)/32)) {
      produce s1 {
        for (i, 0, 32) {
          if (likely(((i.outer*32) &lt; (n - i)))) {
            s1[i] = (scan[(((i.outer*32) + (scan.idx*n)) + i)]*2.000000f)
          }
        }
      }
      for (i.inner, 0, 32) {
        if (likely(((i.outer*32) &lt; (n - i.inner)))) {
          scan[(((i.outer*32) + ((scan.idx + 1)*n)) + i.inner)] = (s1[i.inner] + X[(((i.outer*32) + ((scan.idx + 1)*n)) + i.inner)])
        }
      }
    }
  }
}
</pre></div>
</div>
</div>
<div class="section" id="multiple-states">
<h2>Multiple States<a class="headerlink" href="#multiple-states" title="Permalink to this headline">¶</a></h2>
<p>For complicated applications like RNN, we might need more than one
recurrent state. Scan support multiple recurrent states.
The following example demonstrates how we can build recurrence with two states.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;n&quot;</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;l&quot;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">s_state1</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">s_state2</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">l</span><span class="p">))</span>
<span class="n">s_init1</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
<span class="n">s_init2</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">l</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">)</span>
<span class="n">s_update1</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="n">s_state1</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
<span class="n">s_update2</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">l</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="n">s_state2</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">s_state1</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">s_scan1</span><span class="p">,</span> <span class="n">s_scan2</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">scan</span><span class="p">([</span><span class="n">s_init1</span><span class="p">,</span> <span class="n">s_init2</span><span class="p">],</span>
                            <span class="p">[</span><span class="n">s_update1</span><span class="p">,</span> <span class="n">s_update2</span><span class="p">],</span>
                            <span class="p">[</span><span class="n">s_state1</span><span class="p">,</span> <span class="n">s_state2</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">X</span><span class="p">])</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">create_schedule</span><span class="p">(</span><span class="n">s_scan1</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">tvm</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">s_scan1</span><span class="p">,</span> <span class="n">s_scan2</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>produce scan {
  for (i, 0, n) {
    scan.v0[i] = X[i]
  }
  for (i, 0, l) {
    scan.v1[i] = 0.000000f
  }
  for (scan.idx, 0, (m + -1)) {
    for (i, 0, n) {
      scan.v0[(((scan.idx + 1)*n) + i)] = (scan.v0[((scan.idx*n) + i)] + X[(((scan.idx + 1)*n) + i)])
    }
    for (i, 0, l) {
      scan.v1[(((scan.idx + 1)*l) + i)] = (scan.v1[((scan.idx*l) + i)] + scan.v0[(scan.idx*n)])
    }
  }
}
</pre></div>
</div>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>This tutorial provides a walk through of scan primitive.</p>
<ul class="simple">
<li>Describe scan with init and update.</li>
<li>Schedule the scan cells as normal schedule.</li>
<li>For complicated workload, use multiple states and steps in scan cell.</li>
</ul>
<p><strong>Total running time of the script:</strong> ( 0 minutes  0.394 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-language-scan-py">
<div class="sphx-glr-download docutils container">
<a class="reference download internal" download="" href="../../_downloads/d0cac6b1c3e6665f0861d382e972124a/scan.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">scan.py</span></code></a></div>
<div class="sphx-glr-download docutils container">
<a class="reference download internal" download="" href="../../_downloads/761ede24727e4ff908c4303162955824/scan.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">scan.ipynb</span></code></a></div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.readthedocs.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="intrin_math.html" class="btn btn-neutral float-right" title="Intrinsics and Math Functions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="reduction.html" class="btn btn-neutral float-left" title="Reduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, tvm developers

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-75982049-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>